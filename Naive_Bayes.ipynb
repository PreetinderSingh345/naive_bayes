{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0970ece05f2fd367a433285c0204e778ad1644066d163bed046b3b0abfdd35b59",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the needed libraries/packages/modules\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit function to train the model and return a corresponding dictionary\n",
    "\n",
    "def fit(X, Y) :\n",
    "    # making a dictionary and storing the total count of data points in it\n",
    "\n",
    "    dict={}\n",
    "    dict[\"total_count\"]=Y.shape[0]\n",
    "\n",
    "    # iterating on the unique class values in the output \n",
    "\n",
    "    class_values=set(Y)\n",
    "    \n",
    "    for class_value in class_values :\n",
    "        # making a dictionary for each class\n",
    "\n",
    "        dict[class_value]={}\n",
    "\n",
    "        # getting input and output corresponding to the current class\n",
    "\n",
    "        X_class_value=X[Y==class_value]\n",
    "        Y_class_value=Y[Y==class_value]\n",
    "\n",
    "        # storing the total count of data points with the current class\n",
    "\n",
    "        dict[class_value][\"total_count\"]=Y_class_value.shape[0]\n",
    "\n",
    "        # iterating on all the features for a particular class\n",
    "\n",
    "        for j in range(X_class_value.shape[1]) :\n",
    "            # making a dictionary for each feature corresponding to the current class\n",
    "\n",
    "            dict[class_value][j]={}\n",
    "\n",
    "            # iterating on the unique values of the current feature for the current class\n",
    "\n",
    "            feature_values=set(X_class_value[:, j])\n",
    "\n",
    "            for feature_value in feature_values :\n",
    "                # setting the count for a particular feature value for a particular class\n",
    "\n",
    "                dict[class_value][j][feature_value]=(X_class_value[:, j]==feature_value).sum()\n",
    "\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probability function which returns the log probability(we're considering the log probability as the multiplication of many small probabilities can lead to zero probability which indicates that x cannot belong to a given class which is a bold assumption, which we should not make) that given data point belongs to a given class\n",
    "\n",
    "def getProb(dict, x, class_value) :\n",
    "    # initializing output with the probability that the class is the current class\n",
    "\n",
    "    output=(np.log(dict[class_value][\"total_count\"])-np.log(dict[\"total_count\"]))        \n",
    "\n",
    "    # iterating on all the features \n",
    "\n",
    "    for j in range(X.shape[1]) :                \n",
    "        # finding the probability that the input data points with the current class have the value of the jth feature similar to that current data point, dict[class_value][j][x[j]] can lead to a key error as the value xj of feature j may not have been encountered in the training, so we include a conditional and add the net log probability to the output(each class in dictionary will have all the features)\n",
    "\n",
    "        count_class_value_xj=(dict[class_value][j][x[j]]+1 if x[j] in dict[class_value][j].keys() else 1)\n",
    "        count_class_value=(dict[class_value][\"total_count\"]+len(dict[class_value][j].keys()))  \n",
    "        \n",
    "        output+=(np.log(count_class_value_xj)-np.log(count_class_value))\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get class function which returns the class which a particular data point belongs to\n",
    "\n",
    "def getClass(dict, x) :\n",
    "    # getting the unqiue classes from the dictionary and then iterating on them\n",
    "\n",
    "    class_values=dict.keys()\n",
    "\n",
    "    # default values for the best probability and the output\n",
    "\n",
    "    best_prob=-1\n",
    "    output=None\n",
    "\n",
    "    for class_value in class_values :\n",
    "        # skipping the total count property \n",
    "\n",
    "        if(class_value==\"total_count\") :\n",
    "            continue\n",
    "\n",
    "        # getting the probability that a particular data point belongs to a particular class and then making the necessary updations\n",
    "        \n",
    "        prob_class_value=getProb(dict, x, class_value)\n",
    "\n",
    "        if(best_prob==-1 or prob_class_value>best_prob) :\n",
    "            best_prob=prob_class_value\n",
    "            output=class_value\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict function which returns the predictions for the test input\n",
    "\n",
    "def predict(dict, X) :\n",
    "    Y_pred=[]\n",
    "\n",
    "    for x in X:   \n",
    "        # getting the class which each test data point belongs to\n",
    "\n",
    "        x_class=getClass(dict, x)\n",
    "        Y_pred.append(x_class)\n",
    "\n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the data and getting the corresponding input and output\n",
    "\n",
    "data=datasets.load_iris()\n",
    "\n",
    "X=data.data\n",
    "Y=data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label column function which lables a particular column and then returns it\n",
    "\n",
    "def labelCol(col) :\n",
    "    # for each column we divide the values into 4 parts(0, 1, 2 and 3), divided by three seperators which are 0.5 mean, mean and 1.5 mean\n",
    "\n",
    "    col_mean=col.mean()\n",
    "\n",
    "    lim_1=(col_mean*0.5)\n",
    "    lim_2=col_mean\n",
    "    lim_3=(col_mean*1.5)\n",
    "\n",
    "    for i in range(len(col)) :\n",
    "        if(col[i]<lim_1) :\n",
    "            col[i]=0\n",
    "        elif(col[i]<lim_2) :\n",
    "            col[i]=1\n",
    "        elif(col[i]<lim_3) :\n",
    "            col[i]=2\n",
    "        else :\n",
    "            col[i]=3\n",
    "\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting each input column into labelled data(needed to apply naive bayes on it, currently we're not using gaussian probability density function to predict the probability for continuous valued input)\n",
    "\n",
    "for j in range(X.shape[1]) :\n",
    "    X[:, j]=labelCol(X[:, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into train and test\n",
    "\n",
    "X_train, X_test, Y_train, Y_test=train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the training data into the naive bayes model and getting the correponding dictionary\n",
    "\n",
    "dict=fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the predictions for the testing data\n",
    "\n",
    "Y_pred=predict(dict, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score function which returns the mean accuracy of the model\n",
    "\n",
    "def score(Y_true, Y_test) :\n",
    "    count=0\n",
    "\n",
    "    for i in range(len(Y_test)) :\n",
    "        if(Y_test[i]==Y_true[i]) :\n",
    "            count+=1\n",
    "\n",
    "    return (count/len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "# getting the score of the naive bayes classification algorithm on the testing data\n",
    "\n",
    "score(Y_test, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       1.00      1.00      1.00        16\n           2       1.00      1.00      1.00         9\n\n    accuracy                           1.00        38\n   macro avg       1.00      1.00      1.00        38\nweighted avg       1.00      1.00      1.00        38\n\n[[13  0  0]\n [ 0 16  0]\n [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "# printing the classification report and the confusion matrix for better understanding\n",
    "\n",
    "print(classification_report(Y_test, Y_pred))\n",
    "print(confusion_matrix(Y_test, Y_pred))"
   ]
  }
 ]
}